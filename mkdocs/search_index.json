{
    "docs": [
        {
            "location": "/", 
            "text": "Welcome to the Spark hackathon!\n\n\nRequest your Environments\n\n\nFor this hackathon your team are provided with a Spark environment to run your analytics and Data Scientist Workbenches for each of the team members. We recommend that you use Data Scientist Workbench to prepare your overall project presentation.\n\n\n\n\nRequired:\n \n As the \nTeam Leader\n, \nRequest your team's Spark Environemnt\n\n\nOptional:\n \n As a team member, \nRequest your own Data Scientist Workbench\n\n\n\n\nConfigure Github\n\n\n\n\nFind your teams project on \nHackSpark's GitHub page\n\n\nClone your teams repository (\nsee instructions here\n)\n\n\n\n\nDownload the Sample Data Set\n\n\nWe provided a sample data set for your team, but you can also bring your own data. To load the sample data set, \nLogin to your Spark Server\n and enter the following commands into the terminal:\n\n\ncd /data\nwget --content-disposition j.mp/BostonHackDataV2\nunzip Boston-Hackathon-May-28-v2.zip\n\n\n\n\nNext you'll likely want to \nload the data into HDFS\n and \nrun your first Spark program\n.", 
            "title": "First Steps"
        }, 
        {
            "location": "/#request-your-environments", 
            "text": "For this hackathon your team are provided with a Spark environment to run your analytics and Data Scientist Workbenches for each of the team members. We recommend that you use Data Scientist Workbench to prepare your overall project presentation.   Required:    As the  Team Leader ,  Request your team's Spark Environemnt  Optional:    As a team member,  Request your own Data Scientist Workbench", 
            "title": "Request your Environments"
        }, 
        {
            "location": "/#configure-github", 
            "text": "Find your teams project on  HackSpark's GitHub page  Clone your teams repository ( see instructions here )", 
            "title": "Configure Github"
        }, 
        {
            "location": "/#download-the-sample-data-set", 
            "text": "We provided a sample data set for your team, but you can also bring your own data. To load the sample data set,  Login to your Spark Server  and enter the following commands into the terminal:  cd /data\nwget --content-disposition j.mp/BostonHackDataV2\nunzip Boston-Hackathon-May-28-v2.zip  Next you'll likely want to  load the data into HDFS  and  run your first Spark program .", 
            "title": "Download the Sample Data Set"
        }, 
        {
            "location": "/slack/", 
            "text": "Slack\n is an awesome platform for team communication and it's our tool of choice for this hackathon. All the communication will happen through Slack!\n\n\nYou should have been invited to this hackathon's Slack team during registration. In case you haven't, please find one of the organizers and ask for help!\n\n\nIf you are on Mac OS, we recommend that you install the official native \napp\n. It is also available on mobile for \niOS\n and \nAndroid\n.", 
            "title": "Slack"
        }, 
        {
            "location": "/resources/", 
            "text": "Free courses on \nBig Data University\n:\n\n\n\n\n\n\nSpark Fundamentals\n\n\n\n\n\n\nBig Data Fundamentals\n\n\n\n\n\n\nHadoop Fundamentals I\n\n\n\n\n\n\nMoving Data into Hadoop\n\n\n\n\n\n\nBig Data Analytics - Demos\n\n\n\n\n\n\n\n\n\n\nIPython tutorials on your \nNotebooks\n in Data Scientist Workbench.", 
            "title": "Resources"
        }, 
        {
            "location": "/environments/spark/", 
            "text": "Overview\n\n\nYour team will get you own Spark Environment, which you will use to analyze data. All members of the team are going to share the same credentials to access this environment. As the \nTeam Leader\n, you will need to distribute the login information to all of the members of your team.\n\n\nYour Spark Environment is comprised of the following cloud services:\n\n\n\n\nFile Upload/Download service\n - Get files from your personal computers to the cloud, and vice-versa.\n\n\nSpark Server\n - Run Spark jobs.\n\n\n\n\nRequesting the Spark Environment for your Team\n\n\nAs the \nTeam Leader\n, you must request the environment \nhere\n.\n\n\nAfter 10-15 minutes of submitting the form, you should receive a welcome e-mail. Follow the instructions in that e-mail to gain access to your Spark Environment.\n\n\nOnce you gained access, you must then share the Spark credentials with all the members of your team.\n\n\nUsing your Spark Environment\n\n\nAccessing your Spark Server\n\n\nGet the credentials from your \nTeam Leader\n.\n\n\nMac OS and Linux\n\n\nOpen Terminal and run the command below, where \nUSERNAME\n is your Demo Cloud username and \nIP-ADDRESS\n is the IP address of your Spark Server.\n\n\n# Where you downloaded the .pem file above.\ncd ~/Downloads\n\nchmod 400 *.pem\n\n# E.g.: ssh -i john.pem john@127.0.0.1\nssh -i USERNAME.pem USERNAME@IP-ADDRESS\n\n\n\n\nWindows\n\n\nInstall \nPuTTY\n. You should then use the following steps to connect. Or just get a Mac. :)\n\n\n\n\n\n\nUse \nPuTTYgen\n to convert your SSH key (\nUSERNAME.pem\n) from PEM format to PPK format (\nUSERNAME.ppk\n).\n\n\n\n\n\n\nOpen \nPuTTY\n and set the following parameters:\n\n\n\n\nSession \n Host Name (or IP address): \nIP-ADDRESS\n\n\nConnection \n Data \n Auto-login username: \nUSERNAME\n\n\nConnection \n SSH \n Auth \n Private key for authentication: \nUSERNAME.ppk\n\n\n\n\n\n\n\n\nClick \"Open\" to access your Spark Server.\n\n\n\n\n\n\nNote that \nUSERNAME\n is your Demo Cloud username and \nIP-ADDRESS\n is the IP address of your Spark Server.\n\n\n\n\n\n\nFrom within your Spark Server you can run shell commands such as \npyspark\n, \nspark-shell\n, and \nhdfs\n.\n\n\nUploading code and data to your Spark Server\n\n\n\n\n\n\nAs the \nTeam Leader\n log in to the \nIM Demo Cloud\n and go to your project page.\n\n\nClick the \"Uploads/Downloads\" link, \nthis will open up a new tab in your browser\n. You will be prompted to enter login, use \nadmin/hackboston\n as your username and password.. As the \nTeam Leader\n, share this link with all members of your team. Use Slack!\n\n\nUpload your files to your Spark server using the Upload/Download page.\n\n\nAs soon as the process completes, you should be able to access your files on your Spark Server in the \n/uploads\n directory.\n\n\n\n\nNote:\n Alternatively you can use the \nwget\n command from within your Spark Server to get files from external websites or servers.\n\n\nDownloading files from your Spark Server to your personal computer\n\n\n\n\nFrom the Spark Server, move any files you want to down to the \n/uploads\n directory.\n\n\nAs the \nTeam Leader\n log in to the \nIM Demo Cloud\n and go to your project page.\n\n\nClick the \"Uploads/Downloads\" link, \nthis will open up a new tab in your browser\n\n\n\n\nYou should see the files you moved from \nStep 1\n in the file listing. Simply click the file to download.", 
            "title": "Spark"
        }, 
        {
            "location": "/environments/spark/#overview", 
            "text": "Your team will get you own Spark Environment, which you will use to analyze data. All members of the team are going to share the same credentials to access this environment. As the  Team Leader , you will need to distribute the login information to all of the members of your team.  Your Spark Environment is comprised of the following cloud services:   File Upload/Download service  - Get files from your personal computers to the cloud, and vice-versa.  Spark Server  - Run Spark jobs.", 
            "title": "Overview"
        }, 
        {
            "location": "/environments/spark/#requesting-the-spark-environment-for-your-team", 
            "text": "As the  Team Leader , you must request the environment  here .  After 10-15 minutes of submitting the form, you should receive a welcome e-mail. Follow the instructions in that e-mail to gain access to your Spark Environment.  Once you gained access, you must then share the Spark credentials with all the members of your team.", 
            "title": "Requesting the Spark Environment for your Team"
        }, 
        {
            "location": "/environments/spark/#using-your-spark-environment", 
            "text": "", 
            "title": "Using your Spark Environment"
        }, 
        {
            "location": "/environments/spark/#accessing-your-spark-server", 
            "text": "Get the credentials from your  Team Leader .  Mac OS and Linux  Open Terminal and run the command below, where  USERNAME  is your Demo Cloud username and  IP-ADDRESS  is the IP address of your Spark Server.  # Where you downloaded the .pem file above.\ncd ~/Downloads\n\nchmod 400 *.pem\n\n# E.g.: ssh -i john.pem john@127.0.0.1\nssh -i USERNAME.pem USERNAME@IP-ADDRESS  Windows  Install  PuTTY . You should then use the following steps to connect. Or just get a Mac. :)    Use  PuTTYgen  to convert your SSH key ( USERNAME.pem ) from PEM format to PPK format ( USERNAME.ppk ).    Open  PuTTY  and set the following parameters:   Session   Host Name (or IP address):  IP-ADDRESS  Connection   Data   Auto-login username:  USERNAME  Connection   SSH   Auth   Private key for authentication:  USERNAME.ppk     Click \"Open\" to access your Spark Server.    Note that  USERNAME  is your Demo Cloud username and  IP-ADDRESS  is the IP address of your Spark Server.    From within your Spark Server you can run shell commands such as  pyspark ,  spark-shell , and  hdfs .", 
            "title": "Accessing your Spark Server"
        }, 
        {
            "location": "/environments/spark/#uploading-code-and-data-to-your-spark-server", 
            "text": "As the  Team Leader  log in to the  IM Demo Cloud  and go to your project page.  Click the \"Uploads/Downloads\" link,  this will open up a new tab in your browser . You will be prompted to enter login, use  admin/hackboston  as your username and password.. As the  Team Leader , share this link with all members of your team. Use Slack!  Upload your files to your Spark server using the Upload/Download page.  As soon as the process completes, you should be able to access your files on your Spark Server in the  /uploads  directory.   Note:  Alternatively you can use the  wget  command from within your Spark Server to get files from external websites or servers.", 
            "title": "Uploading code and data to your Spark Server"
        }, 
        {
            "location": "/environments/spark/#downloading-files-from-your-spark-server-to-your-personal-computer", 
            "text": "From the Spark Server, move any files you want to down to the  /uploads  directory.  As the  Team Leader  log in to the  IM Demo Cloud  and go to your project page.  Click the \"Uploads/Downloads\" link,  this will open up a new tab in your browser   You should see the files you moved from  Step 1  in the file listing. Simply click the file to download.", 
            "title": "Downloading files from your Spark Server to your personal computer"
        }, 
        {
            "location": "/environments/dswb/", 
            "text": "Overview\n\n\nData Scientist Workbench aims to be your one-stop shop for data science tools. At this time, it is a technology preview limited to IPython/Jupyter notebooks. We recommend that you use notebooks as the way to visualize, document and present your analysis. Currently, notebooks can be written in either Python or R.\n\n\nRequesting your own Data Scientist Workbench\n\n\nGo to \nData Scientist Workbench\n and click the big blue button.\n\n\nSharing Notebooks\n\n\nSharing notebooks can be very useful not only for team collaboration, but as a means to present your results publicly.\n\n\nSharing a notebook\n\n\n\n\nGo to \nData Scientist Workbench\n.\n\n\nOn the menu, click \"My Notebooks\".\n\n\nFind the notebook you want to share, and click the twistie \n to the left of it.\n\n\nClick the \nshare\n button \n\n\nClick the \n\"Show Link\"\n link to get the link to the shared notebook.\n\n\n\n\nImporting Notebooks\n\n\n\n\nCopy the link of the notebook you want to import (from step 5 ).\n\n\nGo to \nData Scientist Workbench\n and click \"My Notbooks\".\n\n\nPaste the link in the text field at the top right corner of the page.\n\n\nHit \"Enter\" to import the notebook.\n\n\n\n\nVisualize your Analysis Results\n\n\n\n\nUser Spark to write results to CSV files.\n\n\nTransfer from HDFS to local filesystem.\n\n\nDownload the files to your laptop using the Uploads/Downloads service.\n\n\nGo to Data Scientist Notebooks and drag/drop your CSV files in it.", 
            "title": "Data Scientist Workbench"
        }, 
        {
            "location": "/environments/dswb/#overview", 
            "text": "Data Scientist Workbench aims to be your one-stop shop for data science tools. At this time, it is a technology preview limited to IPython/Jupyter notebooks. We recommend that you use notebooks as the way to visualize, document and present your analysis. Currently, notebooks can be written in either Python or R.", 
            "title": "Overview"
        }, 
        {
            "location": "/environments/dswb/#requesting-your-own-data-scientist-workbench", 
            "text": "Go to  Data Scientist Workbench  and click the big blue button.", 
            "title": "Requesting your own Data Scientist Workbench"
        }, 
        {
            "location": "/environments/dswb/#sharing-notebooks", 
            "text": "Sharing notebooks can be very useful not only for team collaboration, but as a means to present your results publicly.", 
            "title": "Sharing Notebooks"
        }, 
        {
            "location": "/environments/dswb/#sharing-a-notebook", 
            "text": "Go to  Data Scientist Workbench .  On the menu, click \"My Notebooks\".  Find the notebook you want to share, and click the twistie   to the left of it.  Click the  share  button   Click the  \"Show Link\"  link to get the link to the shared notebook.", 
            "title": "Sharing a notebook"
        }, 
        {
            "location": "/environments/dswb/#importing-notebooks", 
            "text": "Copy the link of the notebook you want to import (from step 5 ).  Go to  Data Scientist Workbench  and click \"My Notbooks\".  Paste the link in the text field at the top right corner of the page.  Hit \"Enter\" to import the notebook.", 
            "title": "Importing Notebooks"
        }, 
        {
            "location": "/environments/dswb/#visualize-your-analysis-results", 
            "text": "User Spark to write results to CSV files.  Transfer from HDFS to local filesystem.  Download the files to your laptop using the Uploads/Downloads service.  Go to Data Scientist Notebooks and drag/drop your CSV files in it.", 
            "title": "Visualize your Analysis Results"
        }, 
        {
            "location": "/howtos/loadhdfs/", 
            "text": "Assuming your data is in \n/data\n, follow these steps:\n\n\n\n\n\n\nCreate a data directory in HDFS.\n\n\nhdfs dfs -mkdir data\n\n\n\n\n\n\n\nLoad the CSV files.\n\n\ncd /data\nhdfs dfs -put file1.csv data/\n\n\n\n\n\n\n\nList files in the HDFS directory.\n\n\nhdfs dfs -ls data", 
            "title": "Load Data into HDFS"
        }, 
        {
            "location": "/howtos/firstspark/", 
            "text": "Your first Spark program will calculate the value of Pi.\n\n\n\n\n\n\nOn your team's Spark Server, create your program directory structure.\n\n\nmkdir -p /code/SparkPi/src/main/scala\n\n\n\n\n\n\n\nUsing \nvi\n, create \n/code/SparkPi/src/main/scala/SparkPi.scala\n with the following content.\n\n\n/* Import the Spark and math packages */\nimport scala.math.random\nimport org.apache.spark._\n\n/* Computes an approximation to pi */\nobject SparkPi {\n  def main(args: Array[String]) {\n    /* Create the SparkConf object */\n    val conf = new SparkConf().setAppName(\"SparkPi\")\n\n    /* Create the SparkContext */\n    val spark = new SparkContext(conf)\n\n    /* Business logic to calculate Pi */\n    val slices = if (args.length \n 0) args(0).toInt else 2\n    val n = math.min(100000L * slices, Int.MaxValue).toInt // Avoid overflow\n    val count = spark.parallelize(1 until n, slices).map { i =\n\n    val x = random * 2 - 1\n    val y = random * 2 - 1\n    if (x*x + y*y \n 1) 1 else 0\n    }.reduce(_+_)\n\n    /* Printing the value of Pi */\n    println(\"Pi is roughly \" + 4.0 * count / n)\n\n    /* Stop the SparkContext */\n    spark.stop()\n  }\n}\n\n\n\n\n\n\n\nUsing \nvi\n again, create \n/code/SparkPi/sparkpi.sbt\n with the following content.\n\n\nname := \"SparkPi Project\"\nversion := \"1.0\"\nscalaVersion := \"2.10.4\"\nlibraryDependencies += \"org.apache.spark\" %% \"spark-core\" % \"1.3.1\"\n\n\n\n\n\n\n\nPackage your program.\n\n\ncd /code/SparkPi\n\n# This will take a while!\nsbt package\n\n\n\n\n\n\n\nRun it!\n\n\nspark-submit \\\n  --class \"SparkPi\" \\\n  --master local[4] \\\n  target/scala-2.10/sparkpi-project_2.10-1.0.jar", 
            "title": "Run your First Spark Program"
        }, 
        {
            "location": "/howtos/configure-spark-verbosity/", 
            "text": "Create the Log4j properties file.\n\n\ncp \\\n  /opt/ibm/spark-1.3.1_IBM_1-bin-2.6.0/conf/log4j.properties.template \\\n  /opt/ibm/spark-1.3.1_IBM_1-bin-2.6.0/conf/log4j.properties\n\n\n\n\n\n\n\nUsing \nvi\n, open the newly created file.\n\n\nvi /opt/ibm/spark-1.3.1_IBM_1-bin-2.6.0/conf/log4j.properties\n\n\n\n\n\n\n\nEdit the following line.\n\n\nlog4j.rootCategory=WARN, console\n\n\n\nYou must replace \nWARN\n, with one of the following options, going from the most verbose to the least verbose.\n\n\n\n\nDEBUG\n\n\nINFO\n\n\nWARN\n\n\nERROR\n\n\nFATAL\n\n\n\n\nWe recommend using \nERROR\n.\n The file contents should match the following snippet.\n\n\n# Set everything to be logged to the console\nlog4j.rootCategory=ERROR, console\nlog4j.appender.console=org.apache.log4j.ConsoleAppender\nlog4j.appender.console.target=System.err\nlog4j.appender.console.layout=org.apache.log4j.PatternLayout\nlog4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n\n\n# Settings to quiet third party logs that are too verbose\nlog4j.logger.org.eclipse.jetty=WARN\nlog4j.logger.org.eclipse.jetty.util.component.AbstractLifeCycle=ERROR\nlog4j.logger.org.apache.spark.repl.SparkIMain$exprTyper=INFO\nlog4j.logger.org.apache.spark.repl.SparkILoop$SparkILoopInterpreter=INFO", 
            "title": "Configure Spark Verbosity"
        }
    ]
}